{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seven Bai  白勝文   sevenbai@gmail.com\n",
    "RAG (Retrieval-Augmented Generation, 檢索增強生成) 是 LLM (Large Language Model 大型語言模型) 非常重要的應用。透過 RAG 技術，LLM 不需要重新訓練就可以回答特定領域的問題，大大提高了 LLM 的實用性。本次分享將帶大家從 text embedding 原理切入，逐步建構一個 RAG 應用，並探討 RAG 的發展趨勢，由淺入深，兩個小時變身 RAG 達人。。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why RAG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM 並不是真的萬事通！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM 的限制\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 訓練資料太舊\n",
    "  - 內部資料不在訓練範圍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 訓練成本高(使用成本低)\n",
    "  - 不容易更新資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In-Context Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 使用成本高(沒有訓練成本)\n",
    "  - 速度慢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 使用成本低(沒有訓練成本)\n",
    "  - 速度快\n",
    "  - 容易更新資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什麼是 Embedding？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文字在多維空間的映射，相近意義的文字在這空間中處於相近的位置\n",
    "\n",
    "  <img src=\"images/embedding-1.png\" width=\"400\">\n",
    "\n",
    "- 詞與詞的相對位置也具有相似意義\n",
    "\n",
    "  <img src=\"images/embedding-2.png\" width=\"800\">\n",
    "\n",
    "- 甚至可以拿來翻譯\n",
    "\n",
    "  <img src=\"images/embedding-3.png\" width=\"600\">\n",
    "\n",
    "- Embedding是用來將抽象的自然語言量化成方便運算的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 OpenAI API 計算 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "openaiclient = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show embeddings\n",
    "embedding = openaiclient.embeddings.create(input = ['今天天氣很好'], model='text-embedding-3-small').data[0].embedding\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 計算兩個 embedding 的相似度\n",
    "def cosine_similarity(a, b):\n",
    "  return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between two embeddings\n",
    "while True:\n",
    "    str1 = input('Enter string1: ')\n",
    "    if str1 == '': break\n",
    "    str2 = input('Enter string2: ')\n",
    "    if str2 == '': break\n",
    "    data = openaiclient.embeddings.create(input = [str1, str2], model='text-embedding-3-small').data\n",
    "    similarity = cosine_similarity(data[0].embedding, data[1].embedding)\n",
    "    print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Cohere API 計算 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show embeddings\n",
    "embedding = co.embed(texts=['今天天氣很好'], model='multilingual-22-12').embeddings\n",
    "print(embedding[0])\n",
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between two embeddings\n",
    "while True:\n",
    "    str1 = input('Enter string1: ')\n",
    "    if str1 == '': break\n",
    "    str2 = input('Enter string2: ')\n",
    "    if str2 == '': break\n",
    "    embedding = co.embed(texts=[str1, str2], model='multilingual-22-12').embeddings\n",
    "    similarity = cosine_similarity(embedding[0], embedding[1])\n",
    "    print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Hugging Face 上的 BGE-M3 計算 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show embeddings\n",
    "embedding = hf.embed_query(\"今天天氣很好\")\n",
    "print(embedding)\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance between two embeddings\n",
    "while True:\n",
    "    str1 = input('Enter string1: ')\n",
    "    if str1 == '': break\n",
    "    str2 = input('Enter string2: ')\n",
    "    if str2 == '': break\n",
    "    embedding1 = hf.embed_query(str1)\n",
    "    embedding2 = hf.embed_query(str2)\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 該用哪一家的 Embedding Model？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Model 有很多選擇，建議選擇支援多語言版本\n",
    "\n",
    "  [使用繁體中文評測各家 Embedding 模型的檢索能力](https://ihower.tw/blog/archives/12167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉比對中英日韓文字 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "allStr = [\"我很開心\", \"I'm very happy\", \"私は非常に満足している\", \"나는 매우 행복 해요\", \"我很傷心\", \"I'm sad\", \"私は悲しいです\", \"나는 슬프다\"]\n",
    "\n",
    "for str1 in allStr:\n",
    "    for str2 in allStr:\n",
    "        data = openaiclient.embeddings.create(input = [str1, str2], model='text-embedding-3-small').data\n",
    "        similarity = cosine_similarity(data[0].embedding, data[1].embedding)\n",
    "        print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)\n",
    "        time.sleep(1.5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "allStr = [\"我很開心\", \"I'm very happy\", \"私は非常に満足している\", \"나는 매우 행복 해요\", \"我很傷心\", \"I'm sad\", \"私は悲しいです\", \"나는 슬프다\"]\n",
    "\n",
    "for str1 in allStr:\n",
    "    for str2 in allStr:\n",
    "        embedding = co.embed(texts=[str1, str2], model='multilingual-22-12').embeddings\n",
    "        similarity = cosine_similarity(embedding[0], embedding[1])\n",
    "        print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)\n",
    "        time.sleep(1.5)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allStr = [\"我很開心\", \"I'm very happy\", \"私は非常に満足している\", \"나는 매우 행복 해요\", \"我很傷心\", \"I'm sad\", \"私は悲しいです\", \"나는 슬프다\"]\n",
    "\n",
    "for str1 in allStr:\n",
    "    for str2 in allStr:\n",
    "        embedding1 = hf.embed_query(str1)\n",
    "        embedding2 = hf.embed_query(str2)\n",
    "        similarity = cosine_similarity(embedding1, embedding2)\n",
    "        print(f'str1: {str1}\\nstr2: {str2}\\nSimilarity: {similarity}', flush=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multilingual.png\" width=\"1000\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
